++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
DATA SUMMARY START:
 I/O:
     Tag          scheme: NoSeg
     MAX SENTENCE LENGTH: 250
     MAX   WORD   LENGTH: -1
     Number   normalized: True
     Word  alphabet size: 0
     Char  alphabet size: 0
     Label alphabet size: 0
     Trans alphabet size: 0
     Word embedding  dir: data/wiki.nl.vec
     Char embedding  dir: None
     Tran embedding  dir: data/glove.6B.100d.txt
     Word embedding size: 300
     Char embedding size: 30
     Tran embedding size: 100
     Norm   word     emb: False
     Norm   char     emb: False
     Norm   tran     emb: False
     Data_bin   file directory: data/data.pickle
     Train  file directory: data/ned.train
     Dev    file directory: data/ned.testa
     Test   file directory: data/ned.testb
     Raw    file directory: None
     Model  file directory: data/lstmcrf
     Loadmodel   directory: None
     Decode file directory: None
     Train instance number: 0
     Dev   instance number: 0
     Test  instance number: 0
     Raw   instance number: 0
     FEATURE num: 0
 ++++++++++++++++++++++++++++++++++++++++
 Model Network:
     Model        use_crf: True
     Model word extractor: LSTM
     Model       use_char: True
     Model char_seq_feature: LSTM
     Model char_hidden_dim: 50
     Model trans_hidden_dim: 200
 ++++++++++++++++++++++++++++++++++++++++
 Training:
     Optimizer: SGD
     Iteration: 100
     BatchSize: 16
     Average  batch   loss: False
 ++++++++++++++++++++++++++++++++++++++++
 Hyperparameters:
     Hyper              lr: 0.015
     Hyper        lr_decay: 0.05
     Hyper         HP_clip: None
     Hyper        momentum: 0.0
     Hyper              l2: 1e-08
     Hyper      hidden_dim: 200
     Hyper         dropout: 0.5
     Hyper      lstm_layer: 2
     Hyper          bilstm: True
     Hyper             GPU: True
DATA SUMMARY END.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
GPU device:0
Seed num: 42
MODEL: train
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
DATA SUMMARY START:
 I/O:
     Tag          scheme: BIO
     MAX SENTENCE LENGTH: 250
     MAX   WORD   LENGTH: -1
     Number   normalized: True
     Word  alphabet size: 34691
     Char  alphabet size: 91
     Label alphabet size: 10
     Trans alphabet size: 2
     Word embedding  dir: data/wiki.nl.vec
     Char embedding  dir: None
     Tran embedding  dir: data/glove.6B.100d.txt
     Word embedding size: 300
     Char embedding size: 30
     Tran embedding size: 100
     Norm   word     emb: False
     Norm   char     emb: False
     Norm   tran     emb: False
     Data_bin   file directory: data/data.pickle
     Train  file directory: data/ned.train
     Dev    file directory: data/ned.testa
     Test   file directory: data/ned.testb
     Raw    file directory: None
     Model  file directory: data/lstmcrf
     Loadmodel   directory: None
     Decode file directory: None
     Train instance number: 15802
     Dev   instance number: 2895
     Test  instance number: 5194
     Raw   instance number: 0
     FEATURE num: 0
 ++++++++++++++++++++++++++++++++++++++++
 Model Network:
     Model        use_crf: True
     Model word extractor: LSTM
     Model       use_char: True
     Model char_seq_feature: LSTM
     Model char_hidden_dim: 50
     Model trans_hidden_dim: 200
 ++++++++++++++++++++++++++++++++++++++++
 Training:
     Optimizer: SGD
     Iteration: 100
     BatchSize: 16
     Average  batch   loss: False
 ++++++++++++++++++++++++++++++++++++++++
 Hyperparameters:
     Hyper              lr: 0.015
     Hyper        lr_decay: 0.05
     Hyper         HP_clip: None
     Hyper        momentum: 0.0
     Hyper              l2: 1e-08
     Hyper      hidden_dim: 200
     Hyper         dropout: 0.5
     Hyper      lstm_layer: 2
     Hyper          bilstm: True
     Hyper             GPU: True
DATA SUMMARY END.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Training model...
build network...
use_char:  True
char feature extractor:  LSTM
use_trans:  True
word feature extractor:  LSTM
use crf:  True
Build word sequence feature extractor: LSTM...
Build word representation...
Build translation sequence feature extractor: LSTM ...
Build char sequence feature extractor: LSTM ...
build CRF...
model:SeqModel(
  (word_hidden): WordSequence(
    (droplstm): Dropout(p=0.5)
    (wordrep): WordRep(
      (trans_feature): TransBiLSTM(
        (trans_drop): Dropout(p=0.5)
        (trans_embeddings): Embedding(2, 100)
        (trans_lstm): LSTM(100, 100, batch_first=True, bidirectional=True)
      )
      (char_feature): CharBiLSTM(
        (char_drop): Dropout(p=0.5)
        (char_embeddings): Embedding(91, 30)
        (char_lstm): LSTM(30, 25, batch_first=True, bidirectional=True)
      )
      (drop): Dropout(p=0.5)
      (word_embedding): Embedding(34691, 300)
      (feature_embeddings): ModuleList()
    )
    (lstm): LSTM(550, 100, num_layers=2, batch_first=True, bidirectional=True)
    (hidden2tag): Linear(in_features=200, out_features=12, bias=True)
  )
  (crf): CRF()
)
Epoch: 1/100
Learning rate is setted as: 0.015
     Instance: 2000; Time: 21.57s; loss: 10498.2218;mapping_loss: 0.0000; acc: 21898.0/24308.0=0.9009
     Instance: 4000; Time: 22.75s; loss: 5640.3823;mapping_loss: 0.0000; acc: 46193.0/50440.0=0.9158
     Instance: 6000; Time: 24.85s; loss: 9429.1410;mapping_loss: 0.0000; acc: 69728.0/76282.0=0.9141
     Instance: 8000; Time: 22.06s; loss: 3531.0785;mapping_loss: 0.0000; acc: 93003.0/100859.0=0.9221
     Instance: 10000; Time: 22.06s; loss: 2777.4902;mapping_loss: 0.0000; acc: 117140.0/126119.0=0.9288
     Instance: 12000; Time: 23.50s; loss: 2494.7024;mapping_loss: 0.0000; acc: 142286.0/152256.0=0.9345
     Instance: 14000; Time: 22.44s; loss: 2080.0211;mapping_loss: 0.0000; acc: 166749.0/177620.0=0.9388
main.py:219: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  word_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile=volatile_flag).long()
main.py:220: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  label_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile=volatile_flag).long()
main.py:225: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  mask = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile=volatile_flag).byte()
main.py:246: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  volatile=volatile_flag).long()
main.py:264: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  volatile=volatile_flag).long()
     Instance: 15802; Time: 20.08s; loss: 1704.7993;mapping_loss: 0.0000; acc: 188647.0/200256.0=0.9420
Epoch: 1 training finished. Time: 179.31s, speed: 88.13st/s,  total loss: 38155.8365326
gold_num =  2616  pred_num =  2483  right_num =  1809
Dev: time: 10.85s, speed: 267.74st/s; acc: 0.9698, p: 0.7286, r: 0.6915, f: 0.7096
Exceed previous best f score: -10
gold_num =  3867  pred_num =  3685  right_num =  2747
Test: time: 18.51s, speed: 281.58st/s; acc: 0.9752, p: 0.7455, r: 0.7104, f: 0.7275
Epoch: 2/100
Learning rate is setted as: 0.0142857142857
     Instance: 2000; Time: 22.42s; loss: 1629.2146;mapping_loss: 0.0000; acc: 24115.0/24823.0=0.9715
     Instance: 4000; Time: 22.31s; loss: 1643.8544;mapping_loss: 0.0000; acc: 48159.0/49589.0=0.9712
     Instance: 6000; Time: 23.42s; loss: 1519.0838;mapping_loss: 0.0000; acc: 73232.0/75290.0=0.9727
     Instance: 8000; Time: 24.21s; loss: 1545.4429;mapping_loss: 0.0000; acc: 98137.0/100833.0=0.9733
     Instance: 10000; Time: 22.32s; loss: 1488.2175;mapping_loss: 0.0000; acc: 123128.0/126485.0=0.9735
     Instance: 12000; Time: 23.27s; loss: 1352.5210;mapping_loss: 0.0000; acc: 148318.0/152280.0=0.9740
     Instance: 14000; Time: 23.04s; loss: 1311.2392;mapping_loss: 0.0000; acc: 172929.0/177457.0=0.9745
     Instance: 15802; Time: 20.03s; loss: 1126.0462;mapping_loss: 0.0000; acc: 195231.0/200256.0=0.9749
Epoch: 2 training finished. Time: 181.01s, speed: 87.30st/s,  total loss: 11615.6195679
