]0;IPython: xiangyuejia/lrner-baseSeed num: None
Random Seed num: 66754
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
DATA SUMMARY START:
 I/O:
     Tag          scheme: BIO
     MAX SENTENCE LENGTH: 250
     MAX   WORD   LENGTH: -1
     Number   normalized: True
     Word  alphabet size: 34691
     Char  alphabet size: 91
     Label alphabet size: 10
     Trans alphabet size: 82214
     Word embedding  dir: data/wiki.nl.vec
     Char embedding  dir: None
     Tran embedding  dir: data/glove.6B.100d.txt
     Word embedding size: 300
     Char embedding size: 30
     Tran embedding size: 100
     Norm   word     emb: False
     Norm   char     emb: False
     Norm   tran     emb: False
   Data bin file directory: data/data.base2.substring.pickle
     Train  file directory: data/ned.train
     Dev    file directory: data/ned.testa
     Test   file directory: data/ned.testb
     Raw    file directory: None
     Model  file directory: save-substring/
     Loadmodel   directory: None
     Decode file directory: None
     Train instance number: 15802
     Dev   instance number: 2895
     Test  instance number: 5194
     Raw   instance number: 0
     FEATURE num: 1
         Fe: feature_1  alphabet  size: 15
         Fe: feature_1  embedding  dir: data/ned_pos.vector30
         Fe: feature_1  embedding size: 30
         Fe: feature_1  norm       emb: False
 ++++++++++++++++++++++++++++++++++++++++
 Model Network:
     Model        use_crf: True
     Model word extractor: LSTM
     Model       use_char: True
     Model char_seq_feature: LSTM
     Model char_hidden_dim: 50
     Model trans_hidden_dim: 200
 ++++++++++++++++++++++++++++++++++++++++
 Training:
     show_loss_per_batch: 100
     save_model: False
     state_training_name: default
     Optimizer: SGD
     Iteration: 600
     BatchSize: 16
     Average  batch   loss: False
 ++++++++++++++++++++++++++++++++++++++++
 Hyperparameters:
     Hyper        seed_num: 66754
     Hyper              lr: 0.015
     Hyper        lr_decay: 0.05
     Hyper            clip: None
     Hyper        momentum: 0.0
     Hyper              l2: 1e-08
     Hyper      hidden_dim: 200
     Hyper         dropout: 0.5
     Hyper      lstm_layer: 2
     Hyper          bilstm: True
     Hyper             GPU: True
DATA SUMMARY END.
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
      substring dir : data/substring/
    bpe_emb_dir dir : None
    pos_emb_dir dir : None
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
      circul time   : 4
      circul deepth : 2
 gather output mode : concat
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
MODEL: decode_prepare
decode prepare ...
build network...
use_char:  True
char feature extractor:  LSTM
use_trans:  True
word feature extractor:  LSTM
use crf:  True
Build word sequence feature extractor: LSTM...
Build word representation...
Build translation sequence feature extractor: LSTM ...
Build char sequence feature extractor: LSTM ...
build CRF...
model:SeqModel(
  (word_hidden): WordSequence(
    (droplstm): Dropout(p=0.5)
    (wordrep): WordRep(
      (trans_feature): TransBiLSTM(
        (trans_drop): Dropout(p=0.5)
        (trans_embeddings): Embedding(82214, 100)
        (trans_lstm): LSTM(100, 100, batch_first=True, bidirectional=True)
      )
      (char_feature): CharBiLSTM(
        (char_drop): Dropout(p=0.5)
        (char_embeddings): Embedding(91, 30)
        (char_lstm): LSTM(30, 25, batch_first=True, bidirectional=True)
      )
      (drop): Dropout(p=0.5)
      (word_embedding): Embedding(34691, 300)
      (feature_embeddings): ModuleList(
        (0): Embedding(15, 30)
      )
    )
    (lstm): LSTM(580, 100, num_layers=2, batch_first=True, bidirectional=True)
    (hidden2tag): Linear(in_features=200, out_features=12, bias=True)
  )
  (crf): CRF()
)
get middle Time: 55.84s
Traceback (most recent call last):
  File "main.py", line 2715, in <module>
    decode_prepare(data)
  File "main.py", line 1008, in decode_prepare
    myDynamicPlanning.select_ans_dict(ans_dict, feats_lists, maxlen=len_exp_max-1)
  File "dynamicPlanning.py", line 36, in select_ans_dict
    print('error in select_ans_dict: {}\n    num_correct:{}, num_feature_ans:{}'.format(k))
IndexError: tuple index out of range

If you suspect this is an IPython bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

gold_num =  3867  pred_num =  3799  right_num =  3330
	LOC: p: 0.9149, r: 0.9161, f: 0.9155, 739, 740, 677
	MISC: p: 0.8640, r: 0.7709, f: 0.8148, 1187, 1059, 915
	PER: p: 0.9067, r: 0.9358, f: 0.9210, 1059, 1093, 991
	ORG: p: 0.8236, r: 0.8469, f: 0.8351, 882, 907, 747
p:0.876546459595, r:0.861132660978, f:0.868771197495
1000
2000
3000
4000
5000
dynamic_planning Time: 2.95s
gold_num =  3867  pred_num =  3799  right_num =  3330
	LOC: p: 0.9149, r: 0.9161, f: 0.9155, 739, 740, 677
	MISC: p: 0.8640, r: 0.7709, f: 0.8148, 1187, 1059, 915
	PER: p: 0.9067, r: 0.9358, f: 0.9210, 1059, 1093, 991
	ORG: p: 0.8236, r: 0.8469, f: 0.8351, 882, 907, 747
p:0.876546459595, r:0.861132660978, f:0.868771197495
checked: ('word', 2, (3, 5)) len is 63391
